{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d2b19fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 01:46:39,583 | INFO | ============================================================\n",
      "2026-01-09 01:46:39,585 | INFO | MIMIC-IV to FHIR Converter - FIXED VERSION WITH PROPER ICU DATA\n",
      "2026-01-09 01:46:39,586 | INFO | Target: 9000 patients\n",
      "2026-01-09 01:46:39,588 | INFO | Strategy: Find patients with hospital stays â‰¥1 day\n",
      "2026-01-09 01:46:39,590 | INFO | ============================================================\n",
      "2026-01-09 01:46:39,591 | INFO | Step 1: Discovering CSV files...\n",
      "2026-01-09 01:46:39,601 | INFO | Found 31 CSV files\n",
      "2026-01-09 01:46:39,602 | INFO | Output will be saved to: C:\\mimic-iv-2.2\\mimic_fhir_9000_output\n",
      "2026-01-09 01:46:39,602 | INFO | \n",
      "Step 2: Finding patients with data (reading 20+ chunks)...\n",
      "2026-01-09 01:46:39,603 | INFO | Finding patients with data using SMART method...\n",
      "2026-01-09 01:46:39,604 | INFO |   Step 1: Finding patients with meaningful admissions...\n",
      "2026-01-09 01:47:27,956 | INFO |     Found 148,287 patients with admissions â‰¥1 day\n",
      "2026-01-09 01:47:27,989 | INFO |     Sampled down to 27,000 patients\n",
      "2026-01-09 01:47:27,990 | INFO |   Step 2: Checking lab events (reading 20 chunks)...\n",
      "2026-01-09 01:47:28,495 | INFO |       Processed 5 chunks, 250,000 total rows\n",
      "2026-01-09 01:47:28,916 | INFO |       Processed 10 chunks, 500,000 total rows\n",
      "2026-01-09 01:47:29,348 | INFO |       Processed 15 chunks, 750,000 total rows\n",
      "2026-01-09 01:47:29,799 | INFO |       Processed 20 chunks, 1,000,000 total rows\n",
      "2026-01-09 01:47:29,801 | INFO |   Step 3: Checking chart events (reading 20 chunks)...\n",
      "2026-01-09 01:47:30,372 | INFO |       Processed 5 chunks, 250,000 total rows\n",
      "2026-01-09 01:47:30,883 | INFO |       Processed 10 chunks, 500,000 total rows\n",
      "2026-01-09 01:47:31,304 | INFO |       Processed 15 chunks, 750,000 total rows\n",
      "2026-01-09 01:47:31,752 | INFO |       Processed 20 chunks, 1,000,000 total rows\n",
      "2026-01-09 01:47:31,753 | INFO |   Step 4: Selecting final patients...\n",
      "2026-01-09 01:47:31,759 | INFO |     Found 229 patients with â‰¥5 observations\n",
      "2026-01-09 01:47:31,762 | INFO |     Selected 229 patients\n",
      "2026-01-09 01:47:31,763 | WARNING | Only found 229 patients, need 9000\n",
      "2026-01-09 01:47:31,764 | WARNING | Being more lenient: including patients with ANY observations...\n",
      "2026-01-09 01:47:31,877 | INFO |     Now have 229 patients\n",
      "2026-01-09 01:47:31,878 | INFO |   Sample statistics (n=50):\n",
      "2026-01-09 01:47:31,879 | INFO |     Avg observations in sample: 582.1\n",
      "2026-01-09 01:47:31,879 | INFO |     Min observations: 14\n",
      "2026-01-09 01:47:31,881 | INFO |     Max observations: 2338\n",
      "2026-01-09 01:47:31,882 | INFO |     Patients with 10+ obs: 50\n",
      "2026-01-09 01:47:32,011 | ERROR | Only found 229 qualified patients.\n",
      "2026-01-09 01:47:32,012 | ERROR | Trying alternative approach...\n",
      "2026-01-09 01:47:32,733 | INFO | Selected 9000 random patients with admissions\n",
      "2026-01-09 01:47:32,734 | INFO | Selected 9000 patients\n",
      "2026-01-09 01:47:32,735 | INFO | \n",
      "Step 3: Extracting mortality and ICU data...\n",
      "2026-01-09 01:47:32,735 | INFO | Extracting mortality and ICU data...\n",
      "2026-01-09 01:47:37,521 | INFO |   Mortality data extracted for 9000 patients\n",
      "2026-01-09 01:47:37,521 | INFO |   Raw mortality rate: 4.43% (399 deaths)\n",
      "2026-01-09 01:47:38,043 | INFO |   ICU data extracted for 9000 patients\n",
      "2026-01-09 01:47:38,045 | INFO |   Raw ICU admission rate: 28.23% (2541 ICU patients)\n",
      "2026-01-09 01:47:38,046 | INFO |   Total ICU stays: 3672\n",
      "2026-01-09 01:47:38,046 | INFO |   Sample patient 10002495: 1 ICU stays, units: ['Coronary Care Unit (CCU)']\n",
      "2026-01-09 01:47:38,046 | INFO |   Sample patient 10007920: 1 ICU stays, units: ['Medical Intensive Care Unit (MICU)']\n",
      "2026-01-09 01:47:38,047 | INFO |   Sample patient 10011427: 2 ICU stays, units: ['Cardiac Vascular Intensive Care Unit (CVICU)', 'Medical Intensive Care Unit (MICU)']\n",
      "2026-01-09 01:47:38,053 | INFO | \n",
      "Step 4: Creating filtered subset CSVs...\n",
      "2026-01-09 01:47:38,054 | INFO | Creating filtered subset CSVs...\n",
      "2026-01-09 01:47:38,054 | INFO |   Filtering patients...\n",
      "2026-01-09 01:47:38,329 | INFO |     Kept 9000 patients\n",
      "2026-01-09 01:47:38,331 | INFO |   Filtering admissions...\n",
      "2026-01-09 01:47:40,851 | INFO |     Kept 20,924 rows\n",
      "2026-01-09 01:47:40,852 | INFO |   Filtering icustays...\n",
      "2026-01-09 01:47:41,162 | INFO |     Kept 3,672 rows\n",
      "2026-01-09 01:47:41,163 | INFO |   Filtering diagnoses_icd...\n",
      "2026-01-09 01:47:44,418 | INFO |     Kept 233,305 rows\n",
      "2026-01-09 01:47:44,419 | INFO |   Filtering procedures_icd...\n",
      "2026-01-09 01:47:45,113 | INFO |     Kept 32,990 rows\n",
      "2026-01-09 01:47:45,114 | INFO |   Filtering prescriptions...\n",
      "2026-01-09 01:49:21,192 | INFO |     Kept 769,660 rows\n",
      "2026-01-09 01:49:21,193 | INFO |   Filtering labevents...\n",
      "2026-01-09 01:55:58,441 | INFO |     Kept 5,459,985 rows\n",
      "2026-01-09 01:55:58,442 | INFO |   Filtering chartevents...\n",
      "2026-01-09 02:17:06,294 | INFO |     Kept 16,157,831 rows\n",
      "2026-01-09 02:17:06,296 | INFO |   Filtering microbiologyevents...\n",
      "2026-01-09 02:17:57,715 | INFO |     Kept 148,069 rows\n",
      "2026-01-09 02:17:57,715 | INFO |   Filtering services...\n",
      "2026-01-09 02:17:59,208 | INFO |     Kept 22,733 rows\n",
      "2026-01-09 02:17:59,238 | INFO | \n",
      "Step 5: Loading subset data for FHIR conversion...\n",
      "2026-01-09 02:19:13,993 | INFO | Loaded: 9000 patients, 5,459,985 labs, 16,157,831 charts\n",
      "2026-01-09 02:19:13,993 | INFO | \n",
      "Step 6: Creating FHIR Bundles...\n",
      "2026-01-09 02:23:10,742 | INFO |   Converted 200/9000 patients...\n",
      "2026-01-09 02:27:16,900 | INFO |   Converted 400/9000 patients...\n",
      "2026-01-09 02:31:02,769 | INFO |   Converted 600/9000 patients...\n",
      "2026-01-09 02:35:16,253 | INFO |   Converted 800/9000 patients...\n",
      "2026-01-09 02:39:39,327 | INFO |   Converted 1000/9000 patients...\n",
      "2026-01-09 02:43:57,968 | INFO |   Converted 1200/9000 patients...\n",
      "2026-01-09 02:48:22,683 | INFO |   Converted 1400/9000 patients...\n",
      "2026-01-09 02:52:07,593 | INFO |   Converted 1600/9000 patients...\n",
      "2026-01-09 02:55:51,969 | INFO |   Converted 1800/9000 patients...\n",
      "2026-01-09 02:59:49,601 | INFO |   Converted 2000/9000 patients...\n",
      "2026-01-09 03:03:57,101 | INFO |   Converted 2200/9000 patients...\n",
      "2026-01-09 03:08:05,931 | INFO |   Converted 2400/9000 patients...\n",
      "2026-01-09 03:12:31,052 | INFO |   Converted 2600/9000 patients...\n",
      "2026-01-09 03:16:34,373 | INFO |   Converted 2800/9000 patients...\n",
      "2026-01-09 03:21:20,031 | INFO |   Converted 3000/9000 patients...\n",
      "2026-01-09 03:25:35,615 | INFO |   Converted 3200/9000 patients...\n",
      "2026-01-09 03:30:30,473 | INFO |   Converted 3400/9000 patients...\n",
      "2026-01-09 03:34:52,023 | INFO |   Converted 3600/9000 patients...\n",
      "2026-01-09 03:38:44,609 | INFO |   Converted 3800/9000 patients...\n",
      "2026-01-09 03:43:10,847 | INFO |   Converted 4000/9000 patients...\n",
      "2026-01-09 03:46:56,888 | INFO |   Converted 4200/9000 patients...\n",
      "2026-01-09 03:50:43,010 | INFO |   Converted 4400/9000 patients...\n",
      "2026-01-09 03:54:20,052 | INFO |   Converted 4600/9000 patients...\n",
      "2026-01-09 03:57:51,331 | INFO |   Converted 4800/9000 patients...\n",
      "2026-01-09 04:01:42,029 | INFO |   Converted 5000/9000 patients...\n",
      "2026-01-09 04:05:09,446 | INFO |   Converted 5200/9000 patients...\n",
      "2026-01-09 04:09:08,555 | INFO |   Converted 5400/9000 patients...\n",
      "2026-01-09 04:13:13,560 | INFO |   Converted 5600/9000 patients...\n",
      "2026-01-09 04:17:10,200 | INFO |   Converted 5800/9000 patients...\n",
      "2026-01-09 04:21:14,874 | INFO |   Converted 6000/9000 patients...\n",
      "2026-01-09 04:24:55,259 | INFO |   Converted 6200/9000 patients...\n",
      "2026-01-09 04:28:51,505 | INFO |   Converted 6400/9000 patients...\n",
      "2026-01-09 04:32:27,896 | INFO |   Converted 6600/9000 patients...\n",
      "2026-01-09 04:36:33,346 | INFO |   Converted 6800/9000 patients...\n",
      "2026-01-09 04:40:27,839 | INFO |   Converted 7000/9000 patients...\n",
      "2026-01-09 04:44:33,971 | INFO |   Converted 7200/9000 patients...\n",
      "2026-01-09 04:49:08,224 | INFO |   Converted 7400/9000 patients...\n",
      "2026-01-09 04:52:49,570 | INFO |   Converted 7600/9000 patients...\n",
      "2026-01-09 04:56:44,065 | INFO |   Converted 7800/9000 patients...\n",
      "2026-01-09 05:00:39,699 | INFO |   Converted 8000/9000 patients...\n",
      "2026-01-09 05:04:26,902 | INFO |   Converted 8200/9000 patients...\n",
      "2026-01-09 05:08:31,930 | INFO |   Converted 8400/9000 patients...\n",
      "2026-01-09 05:12:58,482 | INFO |   Converted 8600/9000 patients...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 05:16:36,269 | INFO |   Converted 8800/9000 patients...\n",
      "2026-01-09 05:20:44,871 | INFO |   Converted 9000/9000 patients...\n",
      "2026-01-09 05:20:44,871 | INFO | \n",
      "Step 7: Creating manifest and summary...\n",
      "2026-01-09 05:20:45,071 | INFO | \n",
      "============================================================\n",
      "2026-01-09 05:20:45,071 | INFO | âœ… CONVERSION COMPLETE!\n",
      "2026-01-09 05:20:45,071 | INFO | ============================================================\n",
      "2026-01-09 05:20:45,087 | INFO | Patients converted: 9000\n",
      "2026-01-09 05:20:45,087 | INFO | Average observations per patient: 412.3\n",
      "2026-01-09 05:20:45,087 | INFO | Mortality rate: 4.43% (399 deaths)\n",
      "2026-01-09 05:20:45,087 | INFO | ICU admission rate: 28.23% (2541 ICU patients)\n",
      "2026-01-09 05:20:45,103 | INFO | Total ICU stays: 3672\n",
      "2026-01-09 05:20:45,103 | INFO | Average ICU stays per ICU patient: 1.4\n",
      "2026-01-09 05:20:45,103 | INFO | Patients with â‰¥10 observations: 8819\n",
      "2026-01-09 05:20:45,103 | INFO | Patients with â‰¥50 observations: 8187\n",
      "2026-01-09 05:20:45,103 | INFO | Total time: 214.1 minutes\n",
      "2026-01-09 05:20:45,121 | INFO | Speed: 42.0 patients/minute\n",
      "2026-01-09 05:20:45,123 | INFO | Output directory: C:\\mimic-iv-2.2\\mimic_fhir_9000_output\n",
      "2026-01-09 05:20:45,123 | INFO | \n",
      "ðŸŽ‰ Your 9000 patient FHIR dataset with proper ICU data is ready for interoperability analysis!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "mimic_to_fhir_9000_FIXED_WITH_MORTALITY.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import math\n",
    "import logging\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- USER CONFIG ----------------\n",
    "BASE_DIR = r\"C:\\mimic-iv-2.2\"   # <- CHANGE this to your local MIMIC folder root\n",
    "\n",
    "# === TARGET: 9000 PATIENTS ===\n",
    "TARGET_PATIENTS = 9000              # UPDATED from 8500 to 9000\n",
    "MIN_OBSERVATIONS = 5               # LOWERED from 10 to 5 for more patients\n",
    "MIN_ADMISSIONS = 1                  # Must have at least 1 admission\n",
    "# ============================================\n",
    "\n",
    "# Dynamic output directory names\n",
    "OUT_DIR = os.path.join(BASE_DIR, f\"mimic_fhir_{TARGET_PATIENTS}_output\")\n",
    "SUBSET_DIR = os.path.join(BASE_DIR, f\"subset_{TARGET_PATIENTS}\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_CLIENTS = 50\n",
    "CHUNK_SIZE = 50000  # For streaming large files\n",
    "SYNTHETIC_ABSOLUTE = False\n",
    "SYNTHETIC_EPOCH = datetime(2000, 1, 1)\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "def ensure_dir(d):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def find_csvs_in_dir(d: str):\n",
    "    \"\"\"Return dict mapping stem -> fullpath for csv files under dir d.\"\"\"\n",
    "    out = {}\n",
    "    for root, _, files in os.walk(d):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(\".csv\"):\n",
    "                out[Path(f).stem] = os.path.join(root, f)\n",
    "    return out\n",
    "\n",
    "def discover_mimic_csvs(base_dir: str):\n",
    "    \"\"\"Discover CSVs in hosp/ and icu/ under base_dir.\"\"\"\n",
    "    csv_map = {}\n",
    "    for folder in (\"hosp\", \"icu\"):\n",
    "        dirpath = os.path.join(base_dir, folder)\n",
    "        if os.path.isdir(dirpath):\n",
    "            csv_map.update(find_csvs_in_dir(dirpath))\n",
    "    return csv_map\n",
    "\n",
    "def find_patients_with_data_smart(csv_map, target_count=9000):\n",
    "    \"\"\"\n",
    "    SMART method to find patients with data:\n",
    "    1. Get ALL patients with admissions\n",
    "    2. Sample from admissions to find patients with hospital stays\n",
    "    3. Check those patients in lab/chart events with MORE sampling\n",
    "    \"\"\"\n",
    "    logging.info(\"Finding patients with data using SMART method...\")\n",
    "    \n",
    "    # Step 1: Get patients with admissions AND reasonable length of stay\n",
    "    logging.info(\"  Step 1: Finding patients with meaningful admissions...\")\n",
    "    df_admissions = pd.read_csv(csv_map[\"admissions\"], low_memory=False, \n",
    "                               usecols=['subject_id', 'hadm_id', 'admittime', 'dischtime'])\n",
    "    \n",
    "    # Calculate length of stay for each admission\n",
    "    df_admissions['admittime_dt'] = pd.to_datetime(df_admissions['admittime'], errors='coerce')\n",
    "    df_admissions['dischtime_dt'] = pd.to_datetime(df_admissions['dischtime'], errors='coerce')\n",
    "    df_admissions['los_days'] = (df_admissions['dischtime_dt'] - df_admissions['admittime_dt']).dt.total_seconds() / 86400\n",
    "    \n",
    "    # Filter patients with at least one admission of 1+ days (more likely to have data)\n",
    "    patients_with_good_admissions = set()\n",
    "    for patient_id, group in df_admissions.groupby('subject_id'):\n",
    "        if (group['los_days'] >= 1).any():  # At least 1 day stay\n",
    "            patients_with_good_admissions.add(patient_id)\n",
    "    \n",
    "    logging.info(f\"    Found {len(patients_with_good_admissions):,} patients with admissions â‰¥1 day\")\n",
    "    \n",
    "    # If we have too many, sample down\n",
    "    if len(patients_with_good_admissions) > target_count * 3:\n",
    "        patients_with_good_admissions = set(random.sample(list(patients_with_good_admissions), target_count * 3))\n",
    "        logging.info(f\"    Sampled down to {len(patients_with_good_admissions):,} patients\")\n",
    "    \n",
    "    # Step 2: Check labevents for these patients - READ MORE CHUNKS!\n",
    "    logging.info(\"  Step 2: Checking lab events (reading 20 chunks)...\")\n",
    "    patient_lab_counts = Counter()\n",
    "    candidate_set = patients_with_good_admissions\n",
    "    \n",
    "    try:\n",
    "        chunks_processed = 0\n",
    "        total_rows = 0\n",
    "        for chunk in pd.read_csv(csv_map[\"labevents\"], chunksize=CHUNK_SIZE, low_memory=False,\n",
    "                                usecols=['subject_id', 'valuenum']):\n",
    "            chunks_processed += 1\n",
    "            total_rows += len(chunk)\n",
    "            \n",
    "            # Filter for our candidate patients\n",
    "            chunk_filtered = chunk[chunk['subject_id'].isin(candidate_set)]\n",
    "            \n",
    "            # Count patients with numeric values\n",
    "            if not chunk_filtered.empty:\n",
    "                # Group by patient and count non-null values\n",
    "                patient_counts = chunk_filtered.groupby('subject_id')['valuenum'].apply(\n",
    "                    lambda x: x.notna().sum()\n",
    "                )\n",
    "                for patient_id, count in patient_counts.items():\n",
    "                    patient_lab_counts[patient_id] += count\n",
    "            \n",
    "            # Log progress\n",
    "            if chunks_processed % 5 == 0:\n",
    "                logging.info(f\"      Processed {chunks_processed} chunks, {total_rows:,} total rows\")\n",
    "            \n",
    "            # READ MORE CHUNKS - 20 instead of 5!\n",
    "            if chunks_processed >= 20:\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"    Error reading labevents: {e}\")\n",
    "    \n",
    "    # Step 3: Check chartevents for these patients - READ MORE CHUNKS!\n",
    "    logging.info(\"  Step 3: Checking chart events (reading 20 chunks)...\")\n",
    "    patient_chart_counts = Counter()\n",
    "    \n",
    "    try:\n",
    "        chunks_processed = 0\n",
    "        total_rows = 0\n",
    "        for chunk in pd.read_csv(csv_map[\"chartevents\"], chunksize=CHUNK_SIZE, low_memory=False,\n",
    "                                usecols=['subject_id', 'valuenum']):\n",
    "            chunks_processed += 1\n",
    "            total_rows += len(chunk)\n",
    "            \n",
    "            chunk_filtered = chunk[chunk['subject_id'].isin(candidate_set)]\n",
    "            \n",
    "            if not chunk_filtered.empty:\n",
    "                patient_counts = chunk_filtered.groupby('subject_id')['valuenum'].apply(\n",
    "                    lambda x: x.notna().sum()\n",
    "                )\n",
    "                for patient_id, count in patient_counts.items():\n",
    "                    patient_chart_counts[patient_id] += count\n",
    "            \n",
    "            if chunks_processed % 5 == 0:\n",
    "                logging.info(f\"      Processed {chunks_processed} chunks, {total_rows:,} total rows\")\n",
    "            \n",
    "            # READ MORE CHUNKS!\n",
    "            if chunks_processed >= 20:\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"    Error reading chartevents: {e}\")\n",
    "    \n",
    "    # Step 4: Combine counts and select patients\n",
    "    logging.info(\"  Step 4: Selecting final patients...\")\n",
    "    \n",
    "    patient_total_counts = []\n",
    "    for patient_id in candidate_set:\n",
    "        total_obs = patient_lab_counts.get(patient_id, 0) + patient_chart_counts.get(patient_id, 0)\n",
    "        if total_obs >= MIN_OBSERVATIONS:\n",
    "            patient_total_counts.append((patient_id, total_obs))\n",
    "    \n",
    "    # Sort by observation count (highest first)\n",
    "    patient_total_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Take the best patients\n",
    "    selected_patients = [pid for pid, count in patient_total_counts[:target_count]]\n",
    "    \n",
    "    logging.info(f\"    Found {len(patient_total_counts)} patients with â‰¥{MIN_OBSERVATIONS} observations\")\n",
    "    logging.info(f\"    Selected {len(selected_patients)} patients\")\n",
    "    \n",
    "    # If still not enough, be even more lenient\n",
    "    if len(selected_patients) < target_count:\n",
    "        logging.warning(f\"Only found {len(selected_patients)} patients, need {target_count}\")\n",
    "        logging.warning(\"Being more lenient: including patients with ANY observations...\")\n",
    "        \n",
    "        # Add patients with any observations\n",
    "        remaining_needed = target_count - len(selected_patients)\n",
    "        additional_candidates = []\n",
    "        \n",
    "        for patient_id in candidate_set:\n",
    "            if patient_id not in selected_patients:\n",
    "                total_obs = patient_lab_counts.get(patient_id, 0) + patient_chart_counts.get(patient_id, 0)\n",
    "                if total_obs > 0:\n",
    "                    additional_candidates.append((patient_id, total_obs))\n",
    "        \n",
    "        additional_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        selected_patients.extend([pid for pid, count in additional_candidates[:remaining_needed]])\n",
    "        \n",
    "        logging.info(f\"    Now have {len(selected_patients)} patients\")\n",
    "    \n",
    "    # Get statistics\n",
    "    if selected_patients:\n",
    "        stats_sample = random.sample(selected_patients, min(50, len(selected_patients)))\n",
    "        sample_stats = []\n",
    "        for pid in stats_sample:\n",
    "            total_obs = patient_lab_counts.get(pid, 0) + patient_chart_counts.get(pid, 0)\n",
    "            sample_stats.append(total_obs)\n",
    "        \n",
    "        logging.info(f\"  Sample statistics (n={len(sample_stats)}):\")\n",
    "        logging.info(f\"    Avg observations in sample: {np.mean(sample_stats):.1f}\")\n",
    "        logging.info(f\"    Min observations: {min(sample_stats)}\")\n",
    "        logging.info(f\"    Max observations: {max(sample_stats)}\")\n",
    "        logging.info(f\"    Patients with 10+ obs: {sum(1 for x in sample_stats if x >= 10)}\")\n",
    "    \n",
    "    return selected_patients\n",
    "\n",
    "def to_iso(dt):\n",
    "    \"\"\"Convert pandas timestamp or string to ISO str; return None on failure.\"\"\"\n",
    "    if pd.isnull(dt):\n",
    "        return None\n",
    "    try:\n",
    "        ts = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isnull(ts):\n",
    "            return None\n",
    "        return ts.isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def compute_offset_days(ref_ts, ts):\n",
    "    \"\"\"Return integer days offset (ts - ref_ts). If ts invalid return None.\"\"\"\n",
    "    if ref_ts is None or ts is None:\n",
    "        return None\n",
    "    try:\n",
    "        a = pd.to_datetime(ref_ts, errors='coerce')\n",
    "        b = pd.to_datetime(ts, errors='coerce')\n",
    "        if pd.isnull(a) or pd.isnull(b):\n",
    "            return None\n",
    "        delta = b - a\n",
    "        # Round to nearest integer day (floor)\n",
    "        return int(math.floor(delta.total_seconds() / 86400.0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def create_filtered_subset(selected_patients, csv_map, subset_dir):\n",
    "    \"\"\"Create filtered CSV files for selected patients.\"\"\"\n",
    "    logging.info(\"Creating filtered subset CSVs...\")\n",
    "    \n",
    "    selected_set = set(selected_patients)\n",
    "    \n",
    "    # Filter each table\n",
    "    tables_to_process = [\n",
    "        'patients', 'admissions', 'icustays', 'diagnoses_icd',\n",
    "        'procedures_icd', 'prescriptions', 'labevents', 'chartevents',\n",
    "        'microbiologyevents', 'services'\n",
    "    ]\n",
    "    \n",
    "    for table in tables_to_process:\n",
    "        if table not in csv_map:\n",
    "            logging.warning(f\"  Skipping {table} - not found\")\n",
    "            continue\n",
    "            \n",
    "        input_path = csv_map[table]\n",
    "        output_path = os.path.join(subset_dir, f\"{table}.csv\")\n",
    "        \n",
    "        logging.info(f\"  Filtering {table}...\")\n",
    "        \n",
    "        try:\n",
    "            # For patients table, just get our selected patients\n",
    "            if table == 'patients':\n",
    "                df_all = pd.read_csv(input_path, low_memory=False)\n",
    "                df_filtered = df_all[df_all['subject_id'].isin(selected_set)]\n",
    "                df_filtered.to_csv(output_path, index=False)\n",
    "                logging.info(f\"    Kept {len(df_filtered)} patients\")\n",
    "                continue\n",
    "            \n",
    "            # For other tables, check if they have subject_id\n",
    "            df_sample = pd.read_csv(input_path, nrows=1, low_memory=False)\n",
    "            has_subject_id = 'subject_id' in df_sample.columns\n",
    "            \n",
    "            if not has_subject_id:\n",
    "                # Table doesn't have subject_id - copy entire file\n",
    "                import shutil\n",
    "                shutil.copy2(input_path, output_path)\n",
    "                logging.info(f\"    Copied entire file (no subject_id)\")\n",
    "                continue\n",
    "            \n",
    "            # For large tables, use chunked processing\n",
    "            if table in ['labevents', 'chartevents']:\n",
    "                first_chunk = True\n",
    "                total_kept = 0\n",
    "                \n",
    "                for chunk in pd.read_csv(input_path, chunksize=CHUNK_SIZE, low_memory=False):\n",
    "                    chunk_filtered = chunk[chunk['subject_id'].isin(selected_set)]\n",
    "                    \n",
    "                    if not chunk_filtered.empty:\n",
    "                        if first_chunk:\n",
    "                            chunk_filtered.to_csv(output_path, index=False, mode='w')\n",
    "                            first_chunk = False\n",
    "                        else:\n",
    "                            chunk_filtered.to_csv(output_path, index=False, mode='a', header=False)\n",
    "                        \n",
    "                        total_kept += len(chunk_filtered)\n",
    "                \n",
    "                logging.info(f\"    Kept {total_kept:,} rows\")\n",
    "            else:\n",
    "                # For smaller tables, read all at once\n",
    "                df_all = pd.read_csv(input_path, low_memory=False)\n",
    "                df_filtered = df_all[df_all['subject_id'].isin(selected_set)]\n",
    "                df_filtered.to_csv(output_path, index=False)\n",
    "                logging.info(f\"    Kept {len(df_filtered):,} rows\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"    Error filtering {table}: {e}\")\n",
    "            # Create empty file as placeholder\n",
    "            pd.DataFrame().to_csv(output_path, index=False)\n",
    "\n",
    "# Updated function to extract mortality and ICU data with proper ICU extraction\n",
    "def extract_mortality_icu_data(csv_map, selected_patients):\n",
    "    \"\"\"\n",
    "    Extract mortality and ICU admission data for selected patients.\n",
    "    Returns dictionaries with patient_id as key.\n",
    "    \"\"\"\n",
    "    logging.info(\"Extracting mortality and ICU data...\")\n",
    "    \n",
    "    selected_set = set(selected_patients)\n",
    "    mortality_data = {}\n",
    "    icu_data = {}\n",
    "    detailed_icu_data = {}  # Store detailed ICU data\n",
    "    \n",
    "    try:\n",
    "        # Load admissions data for mortality\n",
    "        df_admissions = pd.read_csv(csv_map[\"admissions\"], low_memory=False,\n",
    "                                   usecols=['subject_id', 'hadm_id', 'hospital_expire_flag'])\n",
    "        \n",
    "        # Filter for selected patients\n",
    "        df_admissions_filtered = df_admissions[df_admissions['subject_id'].isin(selected_set)]\n",
    "        \n",
    "        # Extract mortality data - check all admissions for each patient\n",
    "        for patient_id in selected_set:\n",
    "            patient_admissions = df_admissions_filtered[df_admissions_filtered['subject_id'] == patient_id]\n",
    "            # If patient has any admission with hospital_expire_flag = 1, they died in hospital\n",
    "            if not patient_admissions.empty and (patient_admissions['hospital_expire_flag'] == 1).any():\n",
    "                mortality_data[patient_id] = 1\n",
    "            else:\n",
    "                mortality_data[patient_id] = 0\n",
    "        \n",
    "        logging.info(f\"  Mortality data extracted for {len(mortality_data)} patients\")\n",
    "        if mortality_data:\n",
    "            mortality_count = sum(mortality_data.values())\n",
    "            mortality_rate = mortality_count / len(mortality_data) * 100\n",
    "            logging.info(f\"  Raw mortality rate: {mortality_rate:.2f}% ({mortality_count} deaths)\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"  Error extracting mortality data: {e}\")\n",
    "        # Initialize with zeros if extraction fails\n",
    "        mortality_data = {pid: 0 for pid in selected_patients}\n",
    "    \n",
    "    try:\n",
    "        # Load ICU stays data - read ALL columns to get proper ICU data\n",
    "        df_icustays = pd.read_csv(csv_map[\"icustays\"], low_memory=False,\n",
    "                                 usecols=['subject_id', 'hadm_id', 'stay_id', 'first_careunit', \n",
    "                                         'intime', 'outtime', 'los'])\n",
    "        \n",
    "        # Filter for selected patients\n",
    "        df_icustays_filtered = df_icustays[df_icustays['subject_id'].isin(selected_set)]\n",
    "        \n",
    "        # Create detailed ICU data structure for each patient\n",
    "        for _, row in df_icustays_filtered.iterrows():\n",
    "            patient_id = row['subject_id']\n",
    "            if patient_id not in detailed_icu_data:\n",
    "                detailed_icu_data[patient_id] = []\n",
    "            \n",
    "            detailed_icu_data[patient_id].append({\n",
    "                'hadm_id': row['hadm_id'],\n",
    "                'stay_id': row['stay_id'],\n",
    "                'first_careunit': row['first_careunit'],\n",
    "                'intime': row['intime'],\n",
    "                'outtime': row['outtime'],\n",
    "                'los': row['los'] if pd.notnull(row['los']) else None\n",
    "            })\n",
    "        \n",
    "        # Create binary ICU admission flag\n",
    "        icu_data = {pid: 1 if pid in detailed_icu_data else 0 for pid in selected_patients}\n",
    "        \n",
    "        logging.info(f\"  ICU data extracted for {len(icu_data)} patients\")\n",
    "        if icu_data:\n",
    "            icu_count = sum(icu_data.values())\n",
    "            icu_rate = icu_count / len(icu_data) * 100\n",
    "            total_icu_stays = sum(len(stays) for stays in detailed_icu_data.values())\n",
    "            logging.info(f\"  Raw ICU admission rate: {icu_rate:.2f}% ({icu_count} ICU patients)\")\n",
    "            logging.info(f\"  Total ICU stays: {total_icu_stays}\")\n",
    "            \n",
    "            # Log sample of ICU details\n",
    "            if detailed_icu_data:\n",
    "                sample_patients = list(detailed_icu_data.keys())[:3]\n",
    "                for pid in sample_patients:\n",
    "                    stays = detailed_icu_data[pid]\n",
    "                    logging.info(f\"  Sample patient {pid}: {len(stays)} ICU stays, units: {[s['first_careunit'] for s in stays]}\")\n",
    "        \n",
    "        return mortality_data, icu_data, detailed_icu_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"  Error extracting ICU data: {e}\")\n",
    "        # Initialize with zeros if extraction fails\n",
    "        icu_data = {pid: 0 for pid in selected_patients}\n",
    "        detailed_icu_data = {}\n",
    "        return mortality_data, icu_data, detailed_icu_data\n",
    "\n",
    "# Main pipeline\n",
    "def main():\n",
    "    logging.info(\"=\"*60)\n",
    "    logging.info(\"MIMIC-IV to FHIR Converter - FIXED VERSION WITH PROPER ICU DATA\")\n",
    "    logging.info(f\"Target: {TARGET_PATIENTS} patients\")\n",
    "    logging.info(f\"Strategy: Find patients with hospital stays â‰¥1 day\")\n",
    "    logging.info(\"=\"*60)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Step 1: Discover files\n",
    "    logging.info(\"Step 1: Discovering CSV files...\")\n",
    "    csv_map = discover_mimic_csvs(BASE_DIR)\n",
    "    if \"patients\" not in csv_map:\n",
    "        logging.error(\"Could not find patients.csv!\")\n",
    "        return\n",
    "    \n",
    "    logging.info(f\"Found {len(csv_map)} CSV files\")\n",
    "    \n",
    "    # Step 2: Create output directories\n",
    "    ensure_dir(SUBSET_DIR)\n",
    "    ensure_dir(OUT_DIR)\n",
    "    logging.info(f\"Output will be saved to: {OUT_DIR}\")\n",
    "    \n",
    "    # Step 3: Find patients with data\n",
    "    logging.info(\"\\nStep 2: Finding patients with data (reading 20+ chunks)...\")\n",
    "    selected_patient_ids = find_patients_with_data_smart(csv_map, TARGET_PATIENTS)\n",
    "    \n",
    "    if len(selected_patient_ids) < TARGET_PATIENTS * 0.5:\n",
    "        logging.error(f\"Only found {len(selected_patient_ids)} qualified patients.\")\n",
    "        logging.error(\"Trying alternative approach...\")\n",
    "        \n",
    "        # Alternative: Just take random patients with admissions\n",
    "        df_admissions = pd.read_csv(csv_map[\"admissions\"], usecols=['subject_id'], low_memory=False)\n",
    "        all_patients_with_admissions = df_admissions['subject_id'].unique().tolist()\n",
    "        \n",
    "        if len(all_patients_with_admissions) >= TARGET_PATIENTS:\n",
    "            selected_patient_ids = random.sample(all_patients_with_admissions, TARGET_PATIENTS)\n",
    "            logging.info(f\"Selected {len(selected_patient_ids)} random patients with admissions\")\n",
    "        else:\n",
    "            logging.error(f\"Not enough patients with admissions. Found: {len(all_patients_with_admissions)}\")\n",
    "            return\n",
    "    \n",
    "    logging.info(f\"Selected {len(selected_patient_ids)} patients\")\n",
    "    \n",
    "    # Step 4: Extract mortality and ICU data with proper ICU extraction\n",
    "    logging.info(\"\\nStep 3: Extracting mortality and ICU data...\")\n",
    "    mortality_data, icu_data, detailed_icu_data = extract_mortality_icu_data(csv_map, selected_patient_ids)\n",
    "    \n",
    "    # Step 5: Create filtered subset\n",
    "    logging.info(\"\\nStep 4: Creating filtered subset CSVs...\")\n",
    "    create_filtered_subset(selected_patient_ids, csv_map, SUBSET_DIR)\n",
    "    \n",
    "    # Step 6: Load subset data\n",
    "    logging.info(\"\\nStep 5: Loading subset data for FHIR conversion...\")\n",
    "    \n",
    "    def load_csv_safe(table_name):\n",
    "        path = os.path.join(SUBSET_DIR, f\"{table_name}.csv\")\n",
    "        if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "            try:\n",
    "                return pd.read_csv(path, low_memory=False)\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"  Could not load {table_name}: {e}\")\n",
    "                return pd.DataFrame()\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Load the data\n",
    "    df_patients = load_csv_safe(\"patients\")\n",
    "    df_admissions = load_csv_safe(\"admissions\")\n",
    "    df_labevents = load_csv_safe(\"labevents\")\n",
    "    df_chartevents = load_csv_safe(\"chartevents\")\n",
    "    df_diagnoses = load_csv_safe(\"diagnoses_icd\")\n",
    "    df_procedures = load_csv_safe(\"procedures_icd\")\n",
    "    df_prescriptions = load_csv_safe(\"prescriptions\")\n",
    "    df_icustays = load_csv_safe(\"icustays\")\n",
    "    \n",
    "    logging.info(f\"Loaded: {len(df_patients)} patients, {len(df_labevents):,} labs, {len(df_chartevents):,} charts\")\n",
    "    \n",
    "    # Step 7: Create FHIR Bundles\n",
    "    logging.info(\"\\nStep 6: Creating FHIR Bundles...\")\n",
    "    \n",
    "    total_converted = 0\n",
    "    patient_stats = []\n",
    "    \n",
    "    # Utility function for resource IDs\n",
    "    def make_id(prefix, *parts):\n",
    "        parts_str = [str(p) for p in parts if p is not None and str(p).strip() != '']\n",
    "        return f\"{prefix}-{'-'.join(parts_str)}\"\n",
    "    \n",
    "    for idx, patient_id in enumerate(selected_patient_ids, 1):\n",
    "        try:\n",
    "            # Get patient info\n",
    "            patient_rows = df_patients[df_patients['subject_id'] == patient_id]\n",
    "            if patient_rows.empty:\n",
    "                continue\n",
    "                \n",
    "            patient_info = patient_rows.iloc[0].to_dict()\n",
    "            \n",
    "            # Find reference time (earliest admission)\n",
    "            patient_admissions = df_admissions[df_admissions['subject_id'] == patient_id]\n",
    "            t0 = None\n",
    "            if not patient_admissions.empty and 'admittime' in patient_admissions.columns:\n",
    "                admittimes = patient_admissions['admittime'].dropna().tolist()\n",
    "                if admittimes:\n",
    "                    try:\n",
    "                        t0_dates = pd.to_datetime(admittimes, errors='coerce')\n",
    "                        valid_dates = t0_dates[~t0_dates.isna()]\n",
    "                        if not valid_dates.empty:\n",
    "                            t0 = valid_dates.min().isoformat()\n",
    "                    except:\n",
    "                        t0 = None\n",
    "            \n",
    "            # Start building FHIR Bundle\n",
    "            entries = []\n",
    "            \n",
    "            # 1. Patient resource\n",
    "            patient_resource = {\n",
    "                \"resourceType\": \"Patient\",\n",
    "                \"id\": make_id(\"patient\", patient_id),\n",
    "                \"identifier\": [{\"system\": \"http://mimic.mit.edu/subject\", \"value\": str(patient_id)}],\n",
    "                \"gender\": str(patient_info.get('gender', '')).lower(),\n",
    "                \"extension\": [\n",
    "                    {\n",
    "                        \"url\": \"http://mimic.mit.edu/fhir/StructureDefinition/deid-anchor-year\",\n",
    "                        \"valueString\": str(patient_info.get('anchor_year', ''))\n",
    "                    },\n",
    "                    {\n",
    "                        \"url\": \"http://mimic.mit.edu/fhir/StructureDefinition/deid-anchor-age\",\n",
    "                        \"valueInteger\": int(patient_info.get('anchor_age')) \n",
    "                        if pd.notnull(patient_info.get('anchor_age')) else None\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            entries.append({\"resource\": patient_resource})\n",
    "            \n",
    "            # 2. Admissions as Encounters\n",
    "            admission_count = 0\n",
    "            for _, adm in patient_admissions.iterrows():\n",
    "                admission_count += 1\n",
    "                hadm_id = adm.get('hadm_id')\n",
    "                if pd.isna(hadm_id):\n",
    "                    continue\n",
    "                    \n",
    "                enc_resource = {\n",
    "                    \"resourceType\": \"Encounter\",\n",
    "                    \"id\": make_id(\"enc\", patient_id, hadm_id),\n",
    "                    \"subject\": {\"reference\": f\"Patient/{patient_resource['id']}\"},\n",
    "                    \"identifier\": [{\"system\": \"http://mimic.mit.edu/hadm\", \"value\": str(hadm_id)}],\n",
    "                    \"period\": {\n",
    "                        \"start\": to_iso(adm.get('admittime')),\n",
    "                        \"end\": to_iso(adm.get('dischtime'))\n",
    "                    },\n",
    "                    \"hospitalization\": {\n",
    "                        \"dischargeDisposition\": {\n",
    "                            \"text\": str(adm.get('discharge_location', ''))\n",
    "                        }\n",
    "                    } if pd.notnull(adm.get('discharge_location')) else {}\n",
    "                }\n",
    "                \n",
    "                # Add hospital mortality flag if available\n",
    "                if 'hospital_expire_flag' in adm and pd.notnull(adm['hospital_expire_flag']):\n",
    "                    expire_flag = int(adm['hospital_expire_flag'])\n",
    "                    if expire_flag == 1:\n",
    "                        enc_resource.setdefault('extension', []).append({\n",
    "                            \"url\": \"http://mimic.mit.edu/fhir/StructureDefinition/hospital-mortality\",\n",
    "                            \"valueBoolean\": True\n",
    "                        })\n",
    "                        # Also add outcome as a condition\n",
    "                        death_condition = {\n",
    "                            \"resourceType\": \"Condition\",\n",
    "                            \"id\": make_id(\"death\", patient_id, hadm_id),\n",
    "                            \"subject\": {\"reference\": f\"Patient/{patient_resource['id']}\"},\n",
    "                            \"code\": {\n",
    "                                \"coding\": [{\n",
    "                                    \"system\": \"http://snomed.info/sct\",\n",
    "                                    \"code\": \"419099009\",\n",
    "                                    \"display\": \"Dead\"\n",
    "                                }]\n",
    "                            },\n",
    "                            \"clinicalStatus\": {\n",
    "                                \"coding\": [{\n",
    "                                    \"system\": \"http://terminology.hl7.org/CodeSystem/condition-clinical\",\n",
    "                                    \"code\": \"inactive\"\n",
    "                                }]\n",
    "                            },\n",
    "                            \"verificationStatus\": {\n",
    "                                \"coding\": [{\n",
    "                                    \"system\": \"http://terminology.hl7.org/CodeSystem/condition-ver-status\",\n",
    "                                    \"code\": \"confirmed\"\n",
    "                                }]\n",
    "                            }\n",
    "                        }\n",
    "                        entries.append({\"resource\": death_condition})\n",
    "                \n",
    "                # Add offset extensions\n",
    "                if t0:\n",
    "                    start_offset = compute_offset_days(t0, adm.get('admittime'))\n",
    "                    end_offset = compute_offset_days(t0, adm.get('dischtime'))\n",
    "                    \n",
    "                    if start_offset is not None:\n",
    "                        enc_resource.setdefault('extension', []).append({\n",
    "                            \"url\": \"http://your-research.org/fhir/StructureDefinition/event-offset-days-start\",\n",
    "                            \"valueInteger\": int(start_offset)\n",
    "                        })\n",
    "                    if end_offset is not None:\n",
    "                        enc_resource.setdefault('extension', []).append({\n",
    "                            \"url\": \"http://your-research.org/fhir/StructureDefinition/event-offset-days-end\",\n",
    "                            \"valueInteger\": int(end_offset)\n",
    "                        })\n",
    "                \n",
    "                entries.append({\"resource\": enc_resource})\n",
    "            \n",
    "            # 3. ICU stays as proper resources with detailed data\n",
    "            icu_count = 0\n",
    "            # Get detailed ICU data for this patient if available\n",
    "            patient_detailed_icu = detailed_icu_data.get(patient_id, [])\n",
    "            \n",
    "            for icu_idx, icu_stay in enumerate(patient_detailed_icu, 1):\n",
    "                icu_count += 1\n",
    "                hadm_id = icu_stay['hadm_id']\n",
    "                stay_id = icu_stay['stay_id']\n",
    "                \n",
    "                # Create ICU Procedure resource (representing ICU stay)\n",
    "                icu_procedure = {\n",
    "                    \"resourceType\": \"Procedure\",\n",
    "                    \"id\": make_id(\"icu\", patient_id, stay_id),\n",
    "                    \"subject\": {\"reference\": f\"Patient/{patient_resource['id']}\"},\n",
    "                    \"status\": \"completed\",\n",
    "                    \"code\": {\n",
    "                        \"coding\": [{\n",
    "                            \"system\": \"http://snomed.info/sct\",\n",
    "                            \"code\": \"308335008\",  # Patient encounter procedure\n",
    "                            \"display\": \"Patient encounter procedure\"\n",
    "                        }],\n",
    "                        \"text\": f\"ICU Stay in {icu_stay['first_careunit']}\"\n",
    "                    },\n",
    "                    \"performedPeriod\": {\n",
    "                        \"start\": to_iso(icu_stay['intime']),\n",
    "                        \"end\": to_iso(icu_stay['outtime'])\n",
    "                    },\n",
    "                    \"extension\": [\n",
    "                        {\n",
    "                            \"url\": \"http://mimic.mit.edu/fhir/StructureDefinition/icu-stay-details\",\n",
    "                            \"extension\": [\n",
    "                                {\n",
    "                                    \"url\": \"stayId\",\n",
    "                                    \"valueString\": str(stay_id)\n",
    "                                },\n",
    "                                {\n",
    "                                    \"url\": \"icuUnit\",\n",
    "                                    \"valueString\": str(icu_stay['first_careunit'])\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                # Add LOS if available\n",
    "                if icu_stay['los'] is not None:\n",
    "                    try:\n",
    "                        icu_procedure['extension'][0]['extension'].append({\n",
    "                            \"url\": \"lengthOfStay\",\n",
    "                            \"valueDecimal\": float(icu_stay['los'])\n",
    "                        })\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # Add offset extensions if reference time available\n",
    "                if t0:\n",
    "                    start_offset = compute_offset_days(t0, icu_stay['intime'])\n",
    "                    end_offset = compute_offset_days(t0, icu_stay['outtime'])\n",
    "                    \n",
    "                    if start_offset is not None:\n",
    "                        icu_procedure.setdefault('extension', []).append({\n",
    "                            \"url\": \"http://your-research.org/fhir/StructureDefinition/event-offset-days-start\",\n",
    "                            \"valueInteger\": int(start_offset)\n",
    "                        })\n",
    "                    if end_offset is not None:\n",
    "                        icu_procedure.setdefault('extension', []).append({\n",
    "                            \"url\": \"http://your-research.org/fhir/StructureDefinition/event-offset-days-end\",\n",
    "                            \"valueInteger\": int(end_offset)\n",
    "                        })\n",
    "                \n",
    "                entries.append({\"resource\": icu_procedure})\n",
    "            \n",
    "            # Add ICU flag to patient if they had any ICU stays\n",
    "            if icu_count > 0:\n",
    "                patient_resource.setdefault('extension', []).append({\n",
    "                    \"url\": \"http://mimic.mit.edu/fhir/StructureDefinition/ever-in-icu\",\n",
    "                    \"valueBoolean\": True\n",
    "                })\n",
    "                \n",
    "                # Add ICU admission count\n",
    "                patient_resource.setdefault('extension', []).append({\n",
    "                    \"url\": \"http://mimic.mit.edu/fhir/StructureDefinition/icu-stay-count\",\n",
    "                    \"valueInteger\": icu_count\n",
    "                })\n",
    "            \n",
    "            # 4. Lab events as Observations\n",
    "            lab_obs_count = 0\n",
    "            patient_labs = df_labevents[df_labevents['subject_id'] == patient_id]\n",
    "            \n",
    "            # Limit to reasonable number (500 per patient)\n",
    "            labs_to_process = patient_labs.head(500)\n",
    "            for lab_idx, (_, lab_row) in enumerate(labs_to_process.iterrows(), 1):\n",
    "                lab_obs_count += 1\n",
    "                \n",
    "                itemid = lab_row.get('itemid')\n",
    "                if pd.isna(itemid):\n",
    "                    continue\n",
    "                \n",
    "                obs_resource = {\n",
    "                    \"resourceType\": \"Observation\",\n",
    "                    \"id\": make_id(\"lab\", patient_id, lab_idx),\n",
    "                    \"subject\": {\"reference\": f\"Patient/{patient_resource['id']}\"},\n",
    "                    \"status\": \"final\",\n",
    "                    \"code\": {\n",
    "                        \"coding\": [{\n",
    "                            \"system\": \"http://mimic.mit.edu/itemid\",\n",
    "                            \"code\": str(itemid)\n",
    "                        }]\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Add value\n",
    "                value = lab_row.get('valuenum')\n",
    "                if pd.notnull(value):\n",
    "                    try:\n",
    "                        obs_resource[\"valueQuantity\"] = {\"value\": float(value)}\n",
    "                        # Add unit if available\n",
    "                        unit = lab_row.get('valueuom') or lab_row.get('unit')\n",
    "                        if pd.notnull(unit):\n",
    "                            obs_resource[\"valueQuantity\"][\"unit\"] = str(unit)\n",
    "                    except:\n",
    "                        obs_resource[\"valueString\"] = str(value)\n",
    "                \n",
    "                # Add timestamp and offset\n",
    "                charttime = lab_row.get('charttime')\n",
    "                if pd.notnull(charttime) and t0:\n",
    "                    offset = compute_offset_days(t0, charttime)\n",
    "                    if offset is not None:\n",
    "                        obs_resource.setdefault('extension', []).append({\n",
    "                            \"url\": \"http://your-research.org/fhir/StructureDefinition/event-offset-days\",\n",
    "                            \"valueInteger\": int(offset)\n",
    "                        })\n",
    "                \n",
    "                entries.append({\"resource\": obs_resource})\n",
    "            \n",
    "            # 5. Chart events as Observations\n",
    "            chart_obs_count = 0\n",
    "            patient_charts = df_chartevents[df_chartevents['subject_id'] == patient_id]\n",
    "            \n",
    "            charts_to_process = patient_charts.head(500)\n",
    "            for chart_idx, (_, chart_row) in enumerate(charts_to_process.iterrows(), 1):\n",
    "                chart_obs_count += 1\n",
    "                \n",
    "                itemid = chart_row.get('itemid')\n",
    "                if pd.isna(itemid):\n",
    "                    continue\n",
    "                \n",
    "                obs_resource = {\n",
    "                    \"resourceType\": \"Observation\",\n",
    "                    \"id\": make_id(\"chart\", patient_id, chart_idx),\n",
    "                    \"subject\": {\"reference\": f\"Patient/{patient_resource['id']}\"},\n",
    "                    \"status\": \"final\",\n",
    "                    \"code\": {\n",
    "                        \"coding\": [{\n",
    "                            \"system\": \"http://mimic.mit.edu/itemid\",\n",
    "                            \"code\": str(itemid)\n",
    "                        }]\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                value = chart_row.get('valuenum')\n",
    "                if pd.notnull(value):\n",
    "                    try:\n",
    "                        obs_resource[\"valueQuantity\"] = {\"value\": float(value)}\n",
    "                        unit = chart_row.get('valueuom') or chart_row.get('unit')\n",
    "                        if pd.notnull(unit):\n",
    "                            obs_resource[\"valueQuantity\"][\"unit\"] = str(unit)\n",
    "                    except:\n",
    "                        obs_resource[\"valueString\"] = str(value)\n",
    "                \n",
    "                charttime = chart_row.get('charttime')\n",
    "                if pd.notnull(charttime) and t0:\n",
    "                    offset = compute_offset_days(t0, charttime)\n",
    "                    if offset is not None:\n",
    "                        obs_resource.setdefault('extension', []).append({\n",
    "                            \"url\": \"http://your-research.org/fhir/StructureDefinition/event-offset-days\",\n",
    "                            \"valueInteger\": int(offset)\n",
    "                        })\n",
    "                \n",
    "                entries.append({\"resource\": obs_resource})\n",
    "            \n",
    "            # 6. Diagnoses as Conditions\n",
    "            patient_diagnoses = df_diagnoses[df_diagnoses['subject_id'] == patient_id]\n",
    "            for diag_idx, (_, diag_row) in enumerate(patient_diagnoses.iterrows(), 1):\n",
    "                icd_code = diag_row.get('icd_code')\n",
    "                if pd.isna(icd_code):\n",
    "                    continue\n",
    "                    \n",
    "                cond_resource = {\n",
    "                    \"resourceType\": \"Condition\",\n",
    "                    \"id\": make_id(\"cond\", patient_id, diag_idx),\n",
    "                    \"subject\": {\"reference\": f\"Patient/{patient_resource['id']}\"},\n",
    "                    \"code\": {\n",
    "                        \"coding\": [{\n",
    "                            \"system\": \"http://hl7.org/fhir/sid/icd-10-cm\",\n",
    "                            \"code\": str(icd_code)\n",
    "                        }]\n",
    "                    },\n",
    "                    \"clinicalStatus\": {\n",
    "                        \"coding\": [{\n",
    "                            \"system\": \"http://terminology.hl7.org/CodeSystem/condition-clinical\",\n",
    "                            \"code\": \"active\"\n",
    "                        }]\n",
    "                    }\n",
    "                }\n",
    "                entries.append({\"resource\": cond_resource})\n",
    "            \n",
    "            # 7. Add mortality outcome flag\n",
    "            mortality_flag = mortality_data.get(patient_id, 0)\n",
    "            if mortality_flag == 1:\n",
    "                patient_resource.setdefault('extension', []).append({\n",
    "                    \"url\": \"http://mimic.mit.edu/fhir/StructureDefinition/hospital-mortality-outcome\",\n",
    "                    \"valueBoolean\": True\n",
    "                })\n",
    "            \n",
    "            # 8. Add ICU admission flag if not already added\n",
    "            icu_flag = icu_data.get(patient_id, 0)\n",
    "            if icu_flag == 1 and icu_count == 0:\n",
    "                # This should not happen if detailed_icu_data is correct\n",
    "                patient_resource.setdefault('extension', []).append({\n",
    "                    \"url\": \"http://mimic.mit.edu/fhir/StructureDefinition/icu-admission-outcome\",\n",
    "                    \"valueBoolean\": True\n",
    "                })\n",
    "            \n",
    "            # 9. Create Bundle\n",
    "            bundle = {\n",
    "                \"resourceType\": \"Bundle\",\n",
    "                \"type\": \"collection\",\n",
    "                \"entry\": entries,\n",
    "                \"meta\": {\n",
    "                    \"source\": \"MIMIC-IV FHIR Conversion\",\n",
    "                    \"profile\": [\"http://hl7.org/fhir/StructureDefinition/Bundle\"],\n",
    "                    \"conversion_time\": datetime.now().isoformat()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Save to file\n",
    "            output_file = os.path.join(OUT_DIR, f\"patient_{patient_id}.json\")\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(bundle, f, indent=2)\n",
    "            \n",
    "            total_converted += 1\n",
    "            patient_stats.append({\n",
    "                'patient_id': patient_id,\n",
    "                'admissions': admission_count,\n",
    "                'icu_stays': icu_count,\n",
    "                'lab_observations': lab_obs_count,\n",
    "                'chart_observations': chart_obs_count,\n",
    "                'total_observations': lab_obs_count + chart_obs_count,\n",
    "                'diagnoses': len(patient_diagnoses),\n",
    "                'mortality': mortality_flag,\n",
    "                'icu_admission': icu_flag\n",
    "            })\n",
    "            \n",
    "            if total_converted % 200 == 0:\n",
    "                logging.info(f\"  Converted {total_converted}/{len(selected_patient_ids)} patients...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error converting patient {patient_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Step 8: Create manifest and summary\n",
    "    logging.info(\"\\nStep 7: Creating manifest and summary...\")\n",
    "    \n",
    "    # Shuffle patients for client assignment\n",
    "    random.Random(RANDOM_SEED).shuffle(selected_patient_ids)\n",
    "    clients = {}\n",
    "    for i in range(N_CLIENTS):\n",
    "        client_id = f\"client_{i+1}\"\n",
    "        client_patients = selected_patient_ids[i::N_CLIENTS]\n",
    "        clients[client_id] = client_patients\n",
    "    \n",
    "    # Create manifest\n",
    "    manifest = {\n",
    "        \"conversion_info\": {\n",
    "            \"target_patients\": TARGET_PATIENTS,\n",
    "            \"actual_patients\": total_converted,\n",
    "            \"conversion_date\": datetime.now().isoformat(),\n",
    "            \"source\": \"MIMIC-IV v2.2\",\n",
    "            \"selection_criteria\": \"Patients with admissions â‰¥1 day and observations\"\n",
    "        },\n",
    "        \"client_distribution\": {k: len(v) for k, v in clients.items()},\n",
    "        \"data_statistics\": {\n",
    "            \"min_observations_per_patient\": MIN_OBSERVATIONS,\n",
    "            \"min_admissions_per_patient\": MIN_ADMISSIONS\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(OUT_DIR, \"manifest.json\"), 'w') as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    \n",
    "    # Create patient-to-client mapping\n",
    "    patient_to_client = {}\n",
    "    for client_id, patient_list in clients.items():\n",
    "        for pid in patient_list:\n",
    "            patient_to_client[str(pid)] = client_id\n",
    "    \n",
    "    with open(os.path.join(OUT_DIR, \"patient_to_client_map.json\"), 'w') as f:\n",
    "        json.dump(patient_to_client, f, indent=2)\n",
    "    \n",
    "    # Create statistics summary with mortality and ICU data\n",
    "    if patient_stats:\n",
    "        stats_df = pd.DataFrame(patient_stats)\n",
    "        \n",
    "        # Calculate mortality and ICU rates\n",
    "        mortality_count = int(stats_df['mortality'].sum())\n",
    "        mortality_rate = mortality_count / len(stats_df) * 100 if len(stats_df) > 0 else 0\n",
    "        \n",
    "        icu_count = int(stats_df['icu_admission'].sum())\n",
    "        icu_rate = icu_count / len(stats_df) * 100 if len(stats_df) > 0 else 0\n",
    "        \n",
    "        # Calculate average ICU stays for those with ICU admissions\n",
    "        icu_patients = stats_df[stats_df['icu_admission'] == 1]\n",
    "        total_icu_stays = int(stats_df['icu_stays'].sum())\n",
    "        avg_icu_stays = icu_patients['icu_stays'].mean() if not icu_patients.empty else 0\n",
    "        \n",
    "        summary = {\n",
    "            \"total_patients_converted\": total_converted,\n",
    "            \"observation_statistics\": {\n",
    "                \"average_per_patient\": float(stats_df['total_observations'].mean()),\n",
    "                \"median_per_patient\": float(stats_df['total_observations'].median()),\n",
    "                \"minimum\": int(stats_df['total_observations'].min()),\n",
    "                \"maximum\": int(stats_df['total_observations'].max()),\n",
    "                \"std_dev\": float(stats_df['total_observations'].std())\n",
    "            },\n",
    "            \"admission_statistics\": {\n",
    "                \"average_per_patient\": float(stats_df['admissions'].mean()),\n",
    "                \"patients_with_multiple_admissions\": int((stats_df['admissions'] > 1).sum())\n",
    "            },\n",
    "            \"icu_statistics\": {\n",
    "                \"icu_patients_count\": icu_count,\n",
    "                \"icu_admission_rate\": float(icu_rate),\n",
    "                \"total_icu_stays\": total_icu_stays,\n",
    "                \"average_icu_stays_per_icu_patient\": float(avg_icu_stays),\n",
    "                \"patients_with_multiple_icu_stays\": int((stats_df['icu_stays'] > 1).sum())\n",
    "            },\n",
    "            \"diagnosis_statistics\": {\n",
    "                \"average_per_patient\": float(stats_df['diagnoses'].mean()),\n",
    "                \"patients_with_diagnoses\": int((stats_df['diagnoses'] > 0).sum())\n",
    "            },\n",
    "            \"outcome_statistics\": {\n",
    "                \"mortality_count\": mortality_count,\n",
    "                \"mortality_rate\": float(mortality_rate),\n",
    "                \"mortality_and_icu\": int((stats_df['mortality'] & stats_df['icu_admission']).sum()),\n",
    "                \"mortality_rate_in_icu_patients\": float(\n",
    "                    (stats_df[stats_df['icu_admission'] == 1]['mortality'].sum() / \n",
    "                     max(1, len(icu_patients))) * 100\n",
    "                )\n",
    "            },\n",
    "            \"data_quality\": {\n",
    "                \"patients_with_10plus_obs\": int((stats_df['total_observations'] >= 10).sum()),\n",
    "                \"patients_with_50plus_obs\": int((stats_df['total_observations'] >= 50).sum()),\n",
    "                \"patients_with_100plus_obs\": int((stats_df['total_observations'] >= 100).sum())\n",
    "            },\n",
    "            \"conversion_metrics\": {\n",
    "                \"total_time_minutes\": round((datetime.now() - start_time).total_seconds() / 60, 1),\n",
    "                \"patients_per_minute\": round(total_converted / max(1, (datetime.now() - start_time).total_seconds() / 60), 1)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(OUT_DIR, \"conversion_summary.json\"), 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        # Print summary\n",
    "        logging.info(\"\\n\" + \"=\"*60)\n",
    "        logging.info(\"âœ… CONVERSION COMPLETE!\")\n",
    "        logging.info(\"=\"*60)\n",
    "        logging.info(f\"Patients converted: {total_converted}\")\n",
    "        logging.info(f\"Average observations per patient: {summary['observation_statistics']['average_per_patient']:.1f}\")\n",
    "        logging.info(f\"Mortality rate: {summary['outcome_statistics']['mortality_rate']:.2f}% ({mortality_count} deaths)\")\n",
    "        logging.info(f\"ICU admission rate: {summary['icu_statistics']['icu_admission_rate']:.2f}% ({icu_count} ICU patients)\")\n",
    "        logging.info(f\"Total ICU stays: {summary['icu_statistics']['total_icu_stays']}\")\n",
    "        logging.info(f\"Average ICU stays per ICU patient: {summary['icu_statistics']['average_icu_stays_per_icu_patient']:.1f}\")\n",
    "        logging.info(f\"Patients with â‰¥10 observations: {summary['data_quality']['patients_with_10plus_obs']}\")\n",
    "        logging.info(f\"Patients with â‰¥50 observations: {summary['data_quality']['patients_with_50plus_obs']}\")\n",
    "        logging.info(f\"Total time: {summary['conversion_metrics']['total_time_minutes']} minutes\")\n",
    "        logging.info(f\"Speed: {summary['conversion_metrics']['patients_per_minute']} patients/minute\")\n",
    "        logging.info(f\"Output directory: {OUT_DIR}\")\n",
    "    \n",
    "    logging.info(f\"\\nðŸŽ‰ Your {TARGET_PATIENTS} patient FHIR dataset with proper ICU data is ready for interoperability analysis!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
